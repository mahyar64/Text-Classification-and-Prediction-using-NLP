{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Text.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahyar64/Text-Classification-and-Prediction-using-NLP/blob/master/NLP_Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "1p5wjcfVFacF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#NLP for text to predict the target\n",
        "\n",
        "For runnign Machine Learning (ML) models on the data for classification, prediction or detection we use these steps:\n",
        "\n",
        "1.   Collecting, loading, and cleaning data\n",
        "2.   Build your model\n",
        "3.   Train the data\n",
        "4.   Evaluation & Prediction\n",
        "\n",
        "###Question \n",
        "Propose a model of ML to read the data from dataset, understand and make a prediction fomr the text which contains the chatbot/call conversation of a Bank costomers. There are two files, 'Training_Dataset.xlsx' for training and 'Validation_dataset.csv' for testing and eveluations. \n",
        "\n",
        "##Implementation platform and reading \n",
        "In the first step, I read and load data locally. For the implementation due to being independent from installing python libraries such as Tensorflow and Keras,\n",
        "I used colaboratory from Google which is a jupyter notebook file (https://colab.research.google.com). \n",
        "Data can be read directly from colab page. \n",
        "\n",
        "##Cleaning and exploring the data-set \n",
        "After reading the dataset, I had a look to see how many categories are in the data set and how many attribute is there for each category.\n",
        "The graphs show the differences.\n",
        "Recent process is done for both training and evaluation data. There are 28 different categories in training data-set.\n",
        "While there are 29 categories in the test set. It means there is a category in the evaluation data that is not represented \n",
        "in training set. I removed it due to make more precise validation and prediction accuracy.\n",
        "The class which is not seen during the training can not be in the test.\n",
        "\n",
        "##Text cleaning and pre-processing\n",
        "Furthermore, I did some text cleaning to improvove the quality of training with removing not useful and not important words and characters\n",
        "such as stop words,  punctuation, bad characters, change text to lower case. A sample of training data-set is presented before and after cleaning text processing.\n",
        "\n",
        "##Build Machine Learning models\n",
        "Subsequently, I defined a pipeline model with using 4 different ML algorithms.\n",
        "\n",
        "1.  Naive Bayes Classifier\n",
        "2.  Logistic Regression\n",
        "3.  Stochastic Gradient Descent (SGD)\n",
        "4.  Neural Network (Deep Learning)\n",
        "\n",
        "The accuracy of prediction for any of above models are 0.58, 0.71, 0.73 and 0.97 respectively. \n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "o30pF5e3FjjC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import random\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "%matplotlib inline\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qql5RZ52GhH0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#files.upload('Training_dataset.xlsx')\n",
        "#!rm Training_dataset.xlsx\n",
        "#!rm Validation_dataset.csv\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Es7XzYvoGmQh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('Training_dataset.xlsx')\n",
        "df = df[pd.notnull(df['TARGET'])]\n",
        "dft=pd.read_csv('Validation_dataset.csv',sep=';')\n",
        "dft = dft[pd.notnull(dft['TARGET'])]\n",
        "print(df.head(10))\n",
        "print(dft.head(10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K802ihycKWV1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "my_Labels = ['activate_card_0','pin_by_sms_0','get_card_limit_0','info_card_types_credit_0','order_card_0','use_abroad_0','info_charges_abroad_0','new_pin_0',\n",
        "           'use_abroad_0f_non_europe','contactless_0','unblock_card_0','cards_overview_0','modify_limit_0','lost_card_0 ', 'activate_card_0','balance_0',\n",
        "           'card_stop_0','use_abroad_0e_europe','new_card_reader_0','use_abroad_0a_credit','info_abroad_0','get_transaction_info_0','use_abroad_0b_debit',\n",
        "           'info_CVC_CVV_0 ','order_card_0e_replacement','info_card_number_0', 'order_card_0d_extra','info_card_expiration_0 '\n",
        "           \n",
        "          ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RePczoBsKecZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(df['TEXT'].apply(lambda x: len(x.split(' '))).sum()) #number of words in the Message column before cleaning"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QbeQDvZ8KnsX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#training set\n",
        "plt.figure(figsize=(10,4))\n",
        "df.TARGET.value_counts().plot(kind='bar');\n",
        "df.TARGET.size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AGhn1z7PK8Eg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#test set\n",
        "plt.figure(figsize=(10,4))\n",
        "dft.TARGET.value_counts().plot(kind='bar');\n",
        "dft.TARGET.size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8upW7rToLeqG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "There is one taget in the test data set that is not in the Training set!! We need to find and clean it from test data set"
      ]
    },
    {
      "metadata": {
        "id": "6relT6YWLueU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dft = dft.replace('inactive_card_0', np.nan).dropna()\n",
        "print(dft.TARGET.value_counts())\n",
        "dft.TARGET.size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sgqz_JQHMFjl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#see some text and \n",
        "def print_plot(index):\n",
        "    example = df[df.index == index][['TEXT','TARGET']].values[0]\n",
        "    if len(example) > 0:\n",
        "        print(example[0])\n",
        "        print(example[1])\n",
        "print_plot(10) # you can print any Attribute here with changing the index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XMdb6QGgMzhB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#print a group of training set\n",
        "for i in range(25 , 35):\n",
        "  print_plot(i)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "truSW3RXNDgw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "text cleaning step includes remove stop words,\n",
        "change text to lower case, remove punctuation, remove bad characters, and so on.\n",
        "'''\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "        text: a string\n",
        "        \n",
        "        return: modified initial string\n",
        "    \"\"\"\n",
        "   \n",
        "    text = text.lower() # lowercase text\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
        "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
        "    return text\n",
        "    \n",
        "df['TEXT'] = df['TEXT'].apply(clean_text) # clean training set\n",
        "dft['TEXT'] = dft['TEXT'].apply(clean_text) # clean test set\n",
        "#df['AdditionalCondition'] = df['AdditionalCondition'].apply(clean_text)\n",
        "\n",
        "print_plot(21) # just an example you can change the index print_plot(index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BuLJ32g7Ngqv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#print a group of training set after cleaning\n",
        "for i in range(25 , 35):\n",
        "  print_plot(i)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KA_zMJWXOKsd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(df['TEXT'].apply(lambda x: len(x.split(' '))).sum()) #number of words in the TEXT column After cleaning training set\n",
        "print(dft['TEXT'].apply(lambda x: len(x.split(' '))).sum()) #number of words in the TEXT column After cleaning test set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F0smwMNNOhd5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train = df.TEXT\n",
        "y_train = df.TARGET\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X_train,y_train , test_size=0.7, random_state = 42)\n",
        "X_test = dft.TEXT\n",
        "y_test = dft.TARGET"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yFs05zg4O2Re",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Naive Bayes Classifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "nb = Pipeline([('vect', CountVectorizer()),\n",
        "               ('tfidf', TfidfTransformer()),\n",
        "               ('clf', MultinomialNB()),\n",
        "              ])\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "y_pred = nb.predict(X_test)\n",
        "\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "# Print a human readable report\n",
        "print(classification_report(y_test, y_pred,target_names=my_Labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VCwi-arWPB8A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logreg = Pipeline([('vect', CountVectorizer()),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
        "               ])\n",
        "\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "print(classification_report(y_test, y_pred,target_names=my_Labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MnqhTyIhQNbx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Stochastic Gradient Descent\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "sgd = Pipeline([('vect', CountVectorizer()),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "                ('clf', SGDClassifier()),\n",
        "               ])\n",
        "#loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None\n",
        "sgd.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = sgd.predict(X_test)\n",
        "\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "print(classification_report(y_test, y_pred,target_names=my_Labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KNwXMbq7QVf1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Neural Network (deep Learning)\n",
        "import itertools\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import utils\n",
        "\n",
        "train_size = int(len(df))\n",
        "train_posts = df['TEXT'][:train_size]\n",
        "train_tags = df['TARGET'][:train_size]\n",
        "\n",
        "test_size = int(len(dft))\n",
        "test_posts = dft['TEXT'][test_size:]\n",
        "test_tags = dft['TARGET'][test_size:]\n",
        "\n",
        "max_words = 1000\n",
        "tokenize = text.Tokenizer(num_words=max_words, char_level=False)\n",
        "tokenize.fit_on_texts(train_posts) # only fit on train\n",
        "\n",
        "x_train = tokenize.texts_to_matrix(train_posts)\n",
        "x_test = tokenize.texts_to_matrix(test_posts)\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(train_tags)\n",
        "y_train = encoder.transform(train_tags)\n",
        "y_test = encoder.transform(test_tags)\n",
        "\n",
        "num_classes = np.max(y_train) + 1\n",
        "y_train = utils.to_categorical(y_train, num_classes)\n",
        "y_test = utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "batch_size = 23\n",
        "epochs = 20\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(max_words,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "              \n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1)\n",
        "model.summary()\n",
        "accuracy=model.evaluate(x_test, y_test)\n",
        "print('Accuracy',accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}